{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hola.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGRXjcwCVHS-",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we train the RNN model that predicts the pronounciation (as phonemes) given some input word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51DKgjEEgx2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.nn.functional import relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacmCTw-hifZ",
        "colab_type": "text"
      },
      "source": [
        "Use a CUDA device if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMUD6n7ohfuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTfjNaIfQElD",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePqZvenLH7mn",
        "colab_type": "text"
      },
      "source": [
        "We mount our Google Drive to access the data files from within this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrX1HvAWH2xM",
        "colab_type": "code",
        "outputId": "80f361d7-7bc4-42d5-f1a5-0d91303a6338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "mounted_folder_fp = '/content/drive'\n",
        "drive.mount(mounted_folder_fp, force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhzHmMNjOcB6",
        "colab_type": "text"
      },
      "source": [
        "We then load all the JSON files we need from the mounted drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMuVqd2espNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('{}/My Drive/Colab Notebooks/hola/data/cmudict-char-ids.json'.format(mounted_folder_fp), 'r', encoding='ascii') as f:\n",
        "    char_ids_dict = json.load(f)\n",
        "\n",
        "with open('{}/My Drive/Colab Notebooks/hola/data/cmudict-phoneme-ids.json'.format(mounted_folder_fp), 'r', encoding='ascii') as f:\n",
        "    phoneme_ids_dict = json.load(f)\n",
        "\n",
        "with open('{}/My Drive/Colab Notebooks/hola/data/cmudict-processed.json'.format(mounted_folder_fp), 'r', encoding='ascii') as f:\n",
        "    cmudict_pairs = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73jb0ZTnfgIF",
        "colab_type": "text"
      },
      "source": [
        "We find the length of the longest word/list of phonemes to define the length of our output PyTorch tensors later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxCI4wUkfpvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max([max(len(pair[0]), len(pair[1])) for pair in cmudict_pairs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbeHjB2uOjRJ",
        "colab_type": "text"
      },
      "source": [
        "We split our pairs into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbmEVNRoh1G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_proportion = 0.9\n",
        "\n",
        "random.shuffle(cmudict_pairs)\n",
        "cmudict_train = cmudict_pairs[:int(train_proportion * len(cmudict_pairs))]\n",
        "cmudict_test = cmudict_pairs[int(train_proportion * len(cmudict_pairs)):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmF7A3PNOUoX",
        "colab_type": "text"
      },
      "source": [
        "# Designing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJPIvTGzO1s9",
        "colab_type": "text"
      },
      "source": [
        "For model architecture, we use a vanilla encoder-decoder RNN model with GRUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya1hhQVuOYct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_inputs, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = torch.reshape(self.embedding(input), (1, 1, -1))\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkJFrhFdSmxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_outputs, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_outputs, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, num_outputs)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = torch.reshape(self.embedding(input), (1, 1, -1))\n",
        "        output = relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9UjJY7WjOJd",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKAqqD_jXwTp",
        "colab_type": "text"
      },
      "source": [
        "The `SOS` and `EOS` tokens will be important later during training, so we give them their special variables now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGevvtY-XvVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = char_ids_dict['SOS']\n",
        "EOS_token = char_ids_dict['EOS']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GC5zEmXxDp",
        "colab_type": "text"
      },
      "source": [
        "For training, we invoke teacher forcing at random in order to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syoma2kPYKX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, loss_fn, teacher_forcing_ratio):\n",
        "    loss = 0\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_optim.zero_grad()\n",
        "    decoder_optim.zero_grad()\n",
        "\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_input = input_tensor[i]\n",
        "        encoder_output, encoder_hidden = encoder(encoder_input, encoder_hidden)\n",
        "        encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    if random.random() < teacher_forcing_ratio:  # if True, invoke teacher forcing\n",
        "        for i in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            decoder_input = target_tensor[i]\n",
        "            loss += loss_fn(decoder_output, target_tensor[i])\n",
        "    else:\n",
        "        for i in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            _, top_index = decoder_output.topk(1)\n",
        "            decoder_input = top_index.squeeze().detach()\n",
        "            loss += loss_fn(decoder_output, target_tensor[i])\n",
        "\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optim.step()\n",
        "    decoder_optim.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk_1J4_hNcNK",
        "colab_type": "text"
      },
      "source": [
        "We want to repeat this training process over our entire training set, simultaneously plotting loss over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCE3J8COctrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.switch_backend('agg')\n",
        "\n",
        "def plot_losses(losses_over_time):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)  # puts ticks at regular intervals\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(losses_over_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HQPigUdV5IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(encoder, decoder, plot_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
        "    losses_over_time = []\n",
        "    curr_loss = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "\n",
        "    for i in range(len(cmudict_train)):\n",
        "        input_char_ids, target_phoneme_ids = cmudict_train[i]\n",
        "        input_tensor = torch.tensor(input_char_ids, device=device).view(-1, 1)\n",
        "        target_tensor = torch.tensor(target_phoneme_ids, device=device).view(-1, 1)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_fn, teacher_forcing_ratio)\n",
        "        curr_loss += loss\n",
        "\n",
        "        if i != 0 and i % plot_every == 0:\n",
        "            avg_curr_loss = curr_loss / plot_every\n",
        "            losses_over_time.append(avg_curr_loss)\n",
        "            curr_loss = 0\n",
        "            print('ITER {} ({:.2f}%) LOSS: {:.4f}'.format(i, i / len(cmudict_train) * 100, avg_curr_loss))\n",
        "\n",
        "    plot_losses(losses_over_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180hR4xgkQ5i",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YN1kamuZGxS",
        "colab_type": "text"
      },
      "source": [
        "It will be important to have a `phoneme_id` to `phoneme` dict, so we set that up now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Hnr9EBZHZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phoneme_ids_inv_dict = {phoneme_id: phoneme for phoneme, phoneme_id in phoneme_ids_dict.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXUQaSn4keYv",
        "colab_type": "text"
      },
      "source": [
        "We use accuracy as the metric to evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff1ymJ23cnuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(input_tensor, encoder, decoder):\n",
        "    decoded_phonemes = []\n",
        "    with torch.no_grad():\n",
        "        input_length = input_tensor.size(0)\n",
        "\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "            encoder_outputs[i] = encoder_output[0, 0]  # = or +=?\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        for i in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            _, top_index = decoder_output.topk(1)\n",
        "            decoder_input = top_index.squeeze().detach()\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                decoded_phonemes.append(phoneme_ids_inv_dict[EOS_token])\n",
        "                break\n",
        "            else:\n",
        "                decoded_phonemes.append(phoneme_ids_inv_dict[decoder_input.item()])\n",
        "\n",
        "    return decoded_phonemes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL-SIu5LkT2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder):\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(cmudict_test)):\n",
        "            input_char_ids, target_phoneme_ids = cmudict_test[i]\n",
        "            input_tensor = torch.tensor(input_char_ids, device=device).view(-1, 1)\n",
        "\n",
        "            predicted_phonemes = predict(input_tensor, encoder, decoder)\n",
        "            actual_phonemes = [phoneme_ids_inv_dict[phoneme_id] for phoneme_id in target_phoneme_ids]\n",
        "            if predicted_phonemes == actual_phonemes:\n",
        "                correct += 1\n",
        "\n",
        "    print('ACCURACY: {:.4f}'.format(correct / len(cmudict_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPEQOqlkU6A",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all Together!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDHOwgv6JfU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwCiIUGffOf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(len(cmudict_train), hidden_size).to(device)\n",
        "decoder = Decoder(len(cmudict_train), hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szkVF3UtyMIx",
        "colab_type": "code",
        "outputId": "3fb9b0ad-cd15-4a22-9e3f-08b59cec1472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_epoch(encoder, decoder)\n",
        "evaluate(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITER 1000 (0.83%) LOSS: 3.5154\n",
            "ITER 2000 (1.66%) LOSS: 2.9508\n",
            "ITER 3000 (2.49%) LOSS: 2.6420\n",
            "ITER 4000 (3.32%) LOSS: 2.4173\n",
            "ITER 5000 (4.15%) LOSS: 2.2404\n",
            "ITER 6000 (4.98%) LOSS: 2.1328\n",
            "ITER 7000 (5.81%) LOSS: 2.0187\n",
            "ITER 8000 (6.64%) LOSS: 1.9603\n",
            "ITER 9000 (7.47%) LOSS: 1.8926\n",
            "ITER 10000 (8.30%) LOSS: 1.7858\n",
            "ITER 11000 (9.13%) LOSS: 1.7015\n",
            "ITER 12000 (9.96%) LOSS: 1.6876\n",
            "ITER 13000 (10.79%) LOSS: 1.6176\n",
            "ITER 14000 (11.62%) LOSS: 1.5512\n",
            "ITER 15000 (12.45%) LOSS: 1.5216\n",
            "ITER 16000 (13.28%) LOSS: 1.5409\n",
            "ITER 17000 (14.11%) LOSS: 1.4714\n",
            "ITER 18000 (14.94%) LOSS: 1.4705\n",
            "ITER 19000 (15.77%) LOSS: 1.4136\n",
            "ITER 20000 (16.60%) LOSS: 1.3780\n",
            "ITER 21000 (17.43%) LOSS: 1.3971\n",
            "ITER 22000 (18.26%) LOSS: 1.3382\n",
            "ITER 23000 (19.09%) LOSS: 1.2932\n",
            "ITER 24000 (19.92%) LOSS: 1.2986\n",
            "ITER 25000 (20.75%) LOSS: 1.2470\n",
            "ITER 26000 (21.58%) LOSS: 1.1845\n",
            "ITER 27000 (22.41%) LOSS: 1.2059\n",
            "ITER 28000 (23.24%) LOSS: 1.2227\n",
            "ITER 29000 (24.07%) LOSS: 1.2116\n",
            "ITER 30000 (24.90%) LOSS: 1.2091\n",
            "ITER 31000 (25.73%) LOSS: 1.1392\n",
            "ITER 32000 (26.56%) LOSS: 1.1528\n",
            "ITER 33000 (27.39%) LOSS: 1.1459\n",
            "ITER 34000 (28.22%) LOSS: 1.1145\n",
            "ITER 35000 (29.05%) LOSS: 1.1786\n",
            "ITER 36000 (29.88%) LOSS: 1.0625\n",
            "ITER 37000 (30.71%) LOSS: 1.0852\n",
            "ITER 38000 (31.54%) LOSS: 1.0899\n",
            "ITER 39000 (32.37%) LOSS: 1.1061\n",
            "ITER 40000 (33.20%) LOSS: 1.0909\n",
            "ITER 41000 (34.03%) LOSS: 1.0919\n",
            "ITER 42000 (34.86%) LOSS: 1.1330\n",
            "ITER 43000 (35.69%) LOSS: 1.0640\n",
            "ITER 44000 (36.52%) LOSS: 1.0912\n",
            "ITER 45000 (37.35%) LOSS: 1.0493\n",
            "ITER 46000 (38.18%) LOSS: 1.0447\n",
            "ITER 47000 (39.01%) LOSS: 1.0482\n",
            "ITER 48000 (39.84%) LOSS: 0.9937\n",
            "ITER 49000 (40.67%) LOSS: 1.0555\n",
            "ITER 50000 (41.50%) LOSS: 1.0382\n",
            "ITER 51000 (42.33%) LOSS: 0.9981\n",
            "ITER 52000 (43.16%) LOSS: 1.0609\n",
            "ITER 53000 (44.00%) LOSS: 0.9788\n",
            "ITER 54000 (44.83%) LOSS: 1.0081\n",
            "ITER 55000 (45.66%) LOSS: 1.0081\n",
            "ITER 56000 (46.49%) LOSS: 1.0135\n",
            "ITER 57000 (47.32%) LOSS: 1.0051\n",
            "ITER 58000 (48.15%) LOSS: 0.9743\n",
            "ITER 59000 (48.98%) LOSS: 0.9253\n",
            "ITER 60000 (49.81%) LOSS: 1.0081\n",
            "ITER 61000 (50.64%) LOSS: 0.9805\n",
            "ITER 62000 (51.47%) LOSS: 0.9733\n",
            "ITER 63000 (52.30%) LOSS: 0.9986\n",
            "ITER 64000 (53.13%) LOSS: 0.9676\n",
            "ITER 65000 (53.96%) LOSS: 0.9491\n",
            "ITER 66000 (54.79%) LOSS: 0.9370\n",
            "ITER 67000 (55.62%) LOSS: 0.9364\n",
            "ITER 68000 (56.45%) LOSS: 0.9613\n",
            "ITER 69000 (57.28%) LOSS: 0.9478\n",
            "ITER 70000 (58.11%) LOSS: 0.9098\n",
            "ITER 71000 (58.94%) LOSS: 0.9049\n",
            "ITER 72000 (59.77%) LOSS: 0.9137\n",
            "ITER 73000 (60.60%) LOSS: 0.9878\n",
            "ITER 74000 (61.43%) LOSS: 0.9409\n",
            "ITER 75000 (62.26%) LOSS: 0.9429\n",
            "ITER 76000 (63.09%) LOSS: 0.9235\n",
            "ITER 77000 (63.92%) LOSS: 0.9351\n",
            "ITER 78000 (64.75%) LOSS: 0.9736\n",
            "ITER 79000 (65.58%) LOSS: 0.9209\n",
            "ITER 80000 (66.41%) LOSS: 0.9416\n",
            "ITER 81000 (67.24%) LOSS: 0.9233\n",
            "ITER 82000 (68.07%) LOSS: 0.8411\n",
            "ITER 83000 (68.90%) LOSS: 0.8845\n",
            "ITER 84000 (69.73%) LOSS: 0.8962\n",
            "ITER 85000 (70.56%) LOSS: 0.8725\n",
            "ITER 86000 (71.39%) LOSS: 0.8623\n",
            "ITER 87000 (72.22%) LOSS: 0.8784\n",
            "ITER 88000 (73.05%) LOSS: 0.9122\n",
            "ITER 89000 (73.88%) LOSS: 0.8755\n",
            "ITER 90000 (74.71%) LOSS: 0.8091\n",
            "ITER 91000 (75.54%) LOSS: 0.9119\n",
            "ITER 92000 (76.37%) LOSS: 0.8637\n",
            "ITER 93000 (77.20%) LOSS: 0.9005\n",
            "ITER 94000 (78.03%) LOSS: 0.8425\n",
            "ITER 95000 (78.86%) LOSS: 0.8523\n",
            "ITER 96000 (79.69%) LOSS: 0.8899\n",
            "ITER 97000 (80.52%) LOSS: 0.8438\n",
            "ITER 98000 (81.35%) LOSS: 0.9083\n",
            "ITER 99000 (82.18%) LOSS: 0.8284\n",
            "ITER 100000 (83.01%) LOSS: 0.8897\n",
            "ITER 101000 (83.84%) LOSS: 0.8815\n",
            "ITER 102000 (84.67%) LOSS: 0.8759\n",
            "ITER 103000 (85.50%) LOSS: 0.8864\n",
            "ITER 104000 (86.33%) LOSS: 0.8778\n",
            "ITER 105000 (87.16%) LOSS: 0.8830\n",
            "ITER 106000 (87.99%) LOSS: 0.8528\n",
            "ITER 107000 (88.82%) LOSS: 0.8700\n",
            "ITER 108000 (89.65%) LOSS: 0.8319\n",
            "ITER 109000 (90.48%) LOSS: 0.8222\n",
            "ITER 110000 (91.31%) LOSS: 0.9158\n",
            "ITER 111000 (92.14%) LOSS: 0.8560\n",
            "ITER 112000 (92.97%) LOSS: 0.8398\n",
            "ITER 113000 (93.80%) LOSS: 0.8260\n",
            "ITER 114000 (94.63%) LOSS: 0.8611\n",
            "ITER 115000 (95.46%) LOSS: 0.8605\n",
            "ITER 116000 (96.29%) LOSS: 0.8338\n",
            "ITER 117000 (97.12%) LOSS: 0.8144\n",
            "ITER 118000 (97.95%) LOSS: 0.8401\n",
            "ITER 119000 (98.78%) LOSS: 0.8203\n",
            "ITER 120000 (99.61%) LOSS: 0.8487\n",
            "ACCURACY: 0.3013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOtKxlP63AMj",
        "colab_type": "code",
        "outputId": "32842948-bef7-42ed-bb35-d959c15b7537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_epoch(encoder, decoder, learning_rate=0.005)\n",
        "evaluate(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITER 1000 (0.83%) LOSS: 0.7679\n",
            "ITER 2000 (1.66%) LOSS: 0.7178\n",
            "ITER 3000 (2.49%) LOSS: 0.7043\n",
            "ITER 4000 (3.32%) LOSS: 0.6727\n",
            "ITER 5000 (4.15%) LOSS: 0.7054\n",
            "ITER 6000 (4.98%) LOSS: 0.6905\n",
            "ITER 7000 (5.81%) LOSS: 0.6878\n",
            "ITER 8000 (6.64%) LOSS: 0.6687\n",
            "ITER 9000 (7.47%) LOSS: 0.6429\n",
            "ITER 10000 (8.30%) LOSS: 0.7154\n",
            "ITER 11000 (9.13%) LOSS: 0.6690\n",
            "ITER 12000 (9.96%) LOSS: 0.6276\n",
            "ITER 13000 (10.79%) LOSS: 0.6628\n",
            "ITER 14000 (11.62%) LOSS: 0.6684\n",
            "ITER 15000 (12.45%) LOSS: 0.6322\n",
            "ITER 16000 (13.28%) LOSS: 0.6424\n",
            "ITER 17000 (14.11%) LOSS: 0.6686\n",
            "ITER 18000 (14.94%) LOSS: 0.6556\n",
            "ITER 19000 (15.77%) LOSS: 0.6702\n",
            "ITER 20000 (16.60%) LOSS: 0.6994\n",
            "ITER 21000 (17.43%) LOSS: 0.6158\n",
            "ITER 22000 (18.26%) LOSS: 0.6326\n",
            "ITER 23000 (19.09%) LOSS: 0.6303\n",
            "ITER 24000 (19.92%) LOSS: 0.6155\n",
            "ITER 25000 (20.75%) LOSS: 0.6055\n",
            "ITER 26000 (21.58%) LOSS: 0.6431\n",
            "ITER 27000 (22.41%) LOSS: 0.6723\n",
            "ITER 28000 (23.24%) LOSS: 0.6599\n",
            "ITER 29000 (24.07%) LOSS: 0.6189\n",
            "ITER 30000 (24.90%) LOSS: 0.6270\n",
            "ITER 31000 (25.73%) LOSS: 0.6422\n",
            "ITER 32000 (26.56%) LOSS: 0.6132\n",
            "ITER 33000 (27.39%) LOSS: 0.6409\n",
            "ITER 34000 (28.22%) LOSS: 0.6611\n",
            "ITER 35000 (29.05%) LOSS: 0.6105\n",
            "ITER 36000 (29.88%) LOSS: 0.6430\n",
            "ITER 37000 (30.71%) LOSS: 0.6066\n",
            "ITER 38000 (31.54%) LOSS: 0.6139\n",
            "ITER 39000 (32.37%) LOSS: 0.6333\n",
            "ITER 40000 (33.20%) LOSS: 0.6480\n",
            "ITER 41000 (34.03%) LOSS: 0.6222\n",
            "ITER 42000 (34.86%) LOSS: 0.6252\n",
            "ITER 43000 (35.69%) LOSS: 0.6391\n",
            "ITER 44000 (36.52%) LOSS: 0.6819\n",
            "ITER 45000 (37.35%) LOSS: 0.6482\n",
            "ITER 46000 (38.18%) LOSS: 0.6017\n",
            "ITER 47000 (39.01%) LOSS: 0.5874\n",
            "ITER 48000 (39.84%) LOSS: 0.6182\n",
            "ITER 49000 (40.67%) LOSS: 0.6315\n",
            "ITER 50000 (41.50%) LOSS: 0.6327\n",
            "ITER 51000 (42.33%) LOSS: 0.6402\n",
            "ITER 52000 (43.16%) LOSS: 0.6266\n",
            "ITER 53000 (44.00%) LOSS: 0.6224\n",
            "ITER 54000 (44.83%) LOSS: 0.6043\n",
            "ITER 55000 (45.66%) LOSS: 0.5871\n",
            "ITER 56000 (46.49%) LOSS: 0.6017\n",
            "ITER 57000 (47.32%) LOSS: 0.5755\n",
            "ITER 58000 (48.15%) LOSS: 0.6048\n",
            "ITER 59000 (48.98%) LOSS: 0.6395\n",
            "ITER 60000 (49.81%) LOSS: 0.6627\n",
            "ITER 61000 (50.64%) LOSS: 0.6144\n",
            "ITER 62000 (51.47%) LOSS: 0.6129\n",
            "ITER 63000 (52.30%) LOSS: 0.6058\n",
            "ITER 64000 (53.13%) LOSS: 0.5804\n",
            "ITER 65000 (53.96%) LOSS: 0.6067\n",
            "ITER 66000 (54.79%) LOSS: 0.5909\n",
            "ITER 67000 (55.62%) LOSS: 0.6052\n",
            "ITER 68000 (56.45%) LOSS: 0.6277\n",
            "ITER 69000 (57.28%) LOSS: 0.6261\n",
            "ITER 70000 (58.11%) LOSS: 0.5988\n",
            "ITER 71000 (58.94%) LOSS: 0.6068\n",
            "ITER 72000 (59.77%) LOSS: 0.6195\n",
            "ITER 73000 (60.60%) LOSS: 0.5669\n",
            "ITER 74000 (61.43%) LOSS: 0.6038\n",
            "ITER 75000 (62.26%) LOSS: 0.5910\n",
            "ITER 76000 (63.09%) LOSS: 0.6412\n",
            "ITER 77000 (63.92%) LOSS: 0.5878\n",
            "ITER 78000 (64.75%) LOSS: 0.6506\n",
            "ITER 79000 (65.58%) LOSS: 0.6276\n",
            "ITER 80000 (66.41%) LOSS: 0.5899\n",
            "ITER 81000 (67.24%) LOSS: 0.6157\n",
            "ITER 82000 (68.07%) LOSS: 0.6150\n",
            "ITER 83000 (68.90%) LOSS: 0.5864\n",
            "ITER 84000 (69.73%) LOSS: 0.6254\n",
            "ITER 85000 (70.56%) LOSS: 0.5658\n",
            "ITER 86000 (71.39%) LOSS: 0.5671\n",
            "ITER 87000 (72.22%) LOSS: 0.6187\n",
            "ITER 88000 (73.05%) LOSS: 0.5709\n",
            "ITER 89000 (73.88%) LOSS: 0.5723\n",
            "ITER 90000 (74.71%) LOSS: 0.5671\n",
            "ITER 91000 (75.54%) LOSS: 0.6261\n",
            "ITER 92000 (76.37%) LOSS: 0.6082\n",
            "ITER 93000 (77.20%) LOSS: 0.6391\n",
            "ITER 94000 (78.03%) LOSS: 0.5930\n",
            "ITER 95000 (78.86%) LOSS: 0.6480\n",
            "ITER 96000 (79.69%) LOSS: 0.6038\n",
            "ITER 97000 (80.52%) LOSS: 0.6079\n",
            "ITER 98000 (81.35%) LOSS: 0.5897\n",
            "ITER 99000 (82.18%) LOSS: 0.5711\n",
            "ITER 100000 (83.01%) LOSS: 0.6149\n",
            "ITER 101000 (83.84%) LOSS: 0.6229\n",
            "ITER 102000 (84.67%) LOSS: 0.5869\n",
            "ITER 103000 (85.50%) LOSS: 0.6112\n",
            "ITER 104000 (86.33%) LOSS: 0.5679\n",
            "ITER 105000 (87.16%) LOSS: 0.6082\n",
            "ITER 106000 (87.99%) LOSS: 0.5995\n",
            "ITER 107000 (88.82%) LOSS: 0.5944\n",
            "ITER 108000 (89.65%) LOSS: 0.5839\n",
            "ITER 109000 (90.48%) LOSS: 0.5878\n",
            "ITER 110000 (91.31%) LOSS: 0.5813\n",
            "ITER 111000 (92.14%) LOSS: 0.6154\n",
            "ITER 112000 (92.97%) LOSS: 0.5518\n",
            "ITER 113000 (93.80%) LOSS: 0.6000\n",
            "ITER 114000 (94.63%) LOSS: 0.5493\n",
            "ITER 115000 (95.46%) LOSS: 0.5267\n",
            "ITER 116000 (96.29%) LOSS: 0.5693\n",
            "ITER 117000 (97.12%) LOSS: 0.5532\n",
            "ITER 118000 (97.95%) LOSS: 0.5960\n",
            "ITER 119000 (98.78%) LOSS: 0.5885\n",
            "ITER 120000 (99.61%) LOSS: 0.5927\n",
            "ACCURACY: 0.4130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT499q9zRT19",
        "colab_type": "code",
        "outputId": "4860cc4f-f4d1-439d-ee91-b5262950ff76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_epoch(encoder, decoder, learning_rate=0.0025, teacher_forcing_ratio=0.25)\n",
        "evaluate(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITER 1000 (0.83%) LOSS: 0.6129\n",
            "ITER 2000 (1.66%) LOSS: 0.5792\n",
            "ITER 3000 (2.49%) LOSS: 0.5752\n",
            "ITER 4000 (3.32%) LOSS: 0.5622\n",
            "ITER 5000 (4.15%) LOSS: 0.5167\n",
            "ITER 6000 (4.98%) LOSS: 0.5598\n",
            "ITER 7000 (5.81%) LOSS: 0.5696\n",
            "ITER 8000 (6.64%) LOSS: 0.5672\n",
            "ITER 9000 (7.47%) LOSS: 0.5494\n",
            "ITER 10000 (8.30%) LOSS: 0.5489\n",
            "ITER 11000 (9.13%) LOSS: 0.5702\n",
            "ITER 12000 (9.96%) LOSS: 0.5351\n",
            "ITER 13000 (10.79%) LOSS: 0.5672\n",
            "ITER 14000 (11.62%) LOSS: 0.5660\n",
            "ITER 15000 (12.45%) LOSS: 0.5590\n",
            "ITER 16000 (13.28%) LOSS: 0.5421\n",
            "ITER 17000 (14.11%) LOSS: 0.5374\n",
            "ITER 18000 (14.94%) LOSS: 0.5410\n",
            "ITER 19000 (15.77%) LOSS: 0.4775\n",
            "ITER 20000 (16.60%) LOSS: 0.5287\n",
            "ITER 21000 (17.43%) LOSS: 0.5500\n",
            "ITER 22000 (18.26%) LOSS: 0.5740\n",
            "ITER 23000 (19.09%) LOSS: 0.5552\n",
            "ITER 24000 (19.92%) LOSS: 0.5663\n",
            "ITER 25000 (20.75%) LOSS: 0.5529\n",
            "ITER 26000 (21.58%) LOSS: 0.5232\n",
            "ITER 27000 (22.41%) LOSS: 0.5524\n",
            "ITER 28000 (23.24%) LOSS: 0.5706\n",
            "ITER 29000 (24.07%) LOSS: 0.5188\n",
            "ITER 30000 (24.90%) LOSS: 0.5550\n",
            "ITER 31000 (25.73%) LOSS: 0.5307\n",
            "ITER 32000 (26.56%) LOSS: 0.5195\n",
            "ITER 33000 (27.39%) LOSS: 0.5510\n",
            "ITER 34000 (28.22%) LOSS: 0.5179\n",
            "ITER 35000 (29.05%) LOSS: 0.5442\n",
            "ITER 36000 (29.88%) LOSS: 0.5146\n",
            "ITER 37000 (30.71%) LOSS: 0.5213\n",
            "ITER 38000 (31.54%) LOSS: 0.5468\n",
            "ITER 39000 (32.37%) LOSS: 0.5605\n",
            "ITER 40000 (33.20%) LOSS: 0.5350\n",
            "ITER 41000 (34.03%) LOSS: 0.5594\n",
            "ITER 42000 (34.86%) LOSS: 0.5380\n",
            "ITER 43000 (35.69%) LOSS: 0.5506\n",
            "ITER 44000 (36.52%) LOSS: 0.5763\n",
            "ITER 45000 (37.35%) LOSS: 0.5480\n",
            "ITER 46000 (38.18%) LOSS: 0.5428\n",
            "ITER 47000 (39.01%) LOSS: 0.5179\n",
            "ITER 48000 (39.84%) LOSS: 0.5771\n",
            "ITER 49000 (40.67%) LOSS: 0.5595\n",
            "ITER 50000 (41.50%) LOSS: 0.5520\n",
            "ITER 51000 (42.33%) LOSS: 0.5352\n",
            "ITER 52000 (43.16%) LOSS: 0.4993\n",
            "ITER 53000 (44.00%) LOSS: 0.5384\n",
            "ITER 54000 (44.83%) LOSS: 0.4911\n",
            "ITER 55000 (45.66%) LOSS: 0.5354\n",
            "ITER 56000 (46.49%) LOSS: 0.5258\n",
            "ITER 57000 (47.32%) LOSS: 0.5436\n",
            "ITER 58000 (48.15%) LOSS: 0.4895\n",
            "ITER 59000 (48.98%) LOSS: 0.5165\n",
            "ITER 60000 (49.81%) LOSS: 0.5888\n",
            "ITER 61000 (50.64%) LOSS: 0.5152\n",
            "ITER 62000 (51.47%) LOSS: 0.5137\n",
            "ITER 63000 (52.30%) LOSS: 0.5356\n",
            "ITER 64000 (53.13%) LOSS: 0.4589\n",
            "ITER 65000 (53.96%) LOSS: 0.5124\n",
            "ITER 66000 (54.79%) LOSS: 0.5431\n",
            "ITER 67000 (55.62%) LOSS: 0.5408\n",
            "ITER 68000 (56.45%) LOSS: 0.5347\n",
            "ITER 69000 (57.28%) LOSS: 0.5439\n",
            "ITER 70000 (58.11%) LOSS: 0.5424\n",
            "ITER 71000 (58.94%) LOSS: 0.5264\n",
            "ITER 72000 (59.77%) LOSS: 0.5434\n",
            "ITER 73000 (60.60%) LOSS: 0.5071\n",
            "ITER 74000 (61.43%) LOSS: 0.5402\n",
            "ITER 75000 (62.26%) LOSS: 0.5344\n",
            "ITER 76000 (63.09%) LOSS: 0.5206\n",
            "ITER 77000 (63.92%) LOSS: 0.5581\n",
            "ITER 78000 (64.75%) LOSS: 0.5045\n",
            "ITER 79000 (65.58%) LOSS: 0.5507\n",
            "ITER 80000 (66.41%) LOSS: 0.5393\n",
            "ITER 81000 (67.24%) LOSS: 0.4877\n",
            "ITER 82000 (68.07%) LOSS: 0.4935\n",
            "ITER 83000 (68.90%) LOSS: 0.5516\n",
            "ITER 84000 (69.73%) LOSS: 0.5297\n",
            "ITER 85000 (70.56%) LOSS: 0.5380\n",
            "ITER 86000 (71.39%) LOSS: 0.5311\n",
            "ITER 87000 (72.22%) LOSS: 0.5367\n",
            "ITER 88000 (73.05%) LOSS: 0.5537\n",
            "ITER 89000 (73.88%) LOSS: 0.5335\n",
            "ITER 90000 (74.71%) LOSS: 0.5268\n",
            "ITER 91000 (75.54%) LOSS: 0.5593\n",
            "ITER 92000 (76.37%) LOSS: 0.5360\n",
            "ITER 93000 (77.20%) LOSS: 0.4967\n",
            "ITER 94000 (78.03%) LOSS: 0.4909\n",
            "ITER 95000 (78.86%) LOSS: 0.5296\n",
            "ITER 96000 (79.69%) LOSS: 0.5053\n",
            "ITER 97000 (80.52%) LOSS: 0.4932\n",
            "ITER 98000 (81.35%) LOSS: 0.5187\n",
            "ITER 99000 (82.18%) LOSS: 0.5390\n",
            "ITER 100000 (83.01%) LOSS: 0.4996\n",
            "ITER 101000 (83.84%) LOSS: 0.5705\n",
            "ITER 102000 (84.67%) LOSS: 0.5510\n",
            "ITER 103000 (85.50%) LOSS: 0.5466\n",
            "ITER 104000 (86.33%) LOSS: 0.5416\n",
            "ITER 105000 (87.16%) LOSS: 0.5153\n",
            "ITER 106000 (87.99%) LOSS: 0.4830\n",
            "ITER 107000 (88.82%) LOSS: 0.5063\n",
            "ITER 108000 (89.65%) LOSS: 0.5180\n",
            "ITER 109000 (90.48%) LOSS: 0.5229\n",
            "ITER 110000 (91.31%) LOSS: 0.5600\n",
            "ITER 111000 (92.14%) LOSS: 0.5288\n",
            "ITER 112000 (92.97%) LOSS: 0.5103\n",
            "ITER 113000 (93.80%) LOSS: 0.5452\n",
            "ITER 114000 (94.63%) LOSS: 0.5365\n",
            "ITER 115000 (95.46%) LOSS: 0.5106\n",
            "ITER 116000 (96.29%) LOSS: 0.5626\n",
            "ITER 117000 (97.12%) LOSS: 0.5429\n",
            "ITER 118000 (97.95%) LOSS: 0.5065\n",
            "ITER 119000 (98.78%) LOSS: 0.5169\n",
            "ITER 120000 (99.61%) LOSS: 0.5099\n",
            "ACCURACY: 0.4730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J1CP_RStTlt",
        "colab_type": "code",
        "outputId": "c150a0c1-eded-49e9-ddf2-48a0ab52c5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_epoch(encoder, decoder, learning_rate=0.001, teacher_forcing_ratio=0.25)\n",
        "evaluate(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITER 1000 (0.83%) LOSS: 0.4585\n",
            "ITER 2000 (1.66%) LOSS: 0.4547\n",
            "ITER 3000 (2.49%) LOSS: 0.4590\n",
            "ITER 4000 (3.32%) LOSS: 0.4611\n",
            "ITER 5000 (4.15%) LOSS: 0.4933\n",
            "ITER 6000 (4.98%) LOSS: 0.4761\n",
            "ITER 7000 (5.81%) LOSS: 0.4783\n",
            "ITER 8000 (6.64%) LOSS: 0.4910\n",
            "ITER 9000 (7.47%) LOSS: 0.4786\n",
            "ITER 10000 (8.30%) LOSS: 0.4493\n",
            "ITER 11000 (9.13%) LOSS: 0.4301\n",
            "ITER 12000 (9.96%) LOSS: 0.5018\n",
            "ITER 13000 (10.79%) LOSS: 0.4570\n",
            "ITER 14000 (11.62%) LOSS: 0.4698\n",
            "ITER 15000 (12.45%) LOSS: 0.4511\n",
            "ITER 16000 (13.28%) LOSS: 0.4855\n",
            "ITER 17000 (14.11%) LOSS: 0.4198\n",
            "ITER 18000 (14.94%) LOSS: 0.4570\n",
            "ITER 19000 (15.77%) LOSS: 0.4565\n",
            "ITER 20000 (16.60%) LOSS: 0.4492\n",
            "ITER 21000 (17.43%) LOSS: 0.4554\n",
            "ITER 22000 (18.26%) LOSS: 0.4881\n",
            "ITER 23000 (19.09%) LOSS: 0.4644\n",
            "ITER 24000 (19.92%) LOSS: 0.4426\n",
            "ITER 25000 (20.75%) LOSS: 0.4610\n",
            "ITER 26000 (21.58%) LOSS: 0.4656\n",
            "ITER 27000 (22.41%) LOSS: 0.4778\n",
            "ITER 28000 (23.24%) LOSS: 0.4804\n",
            "ITER 29000 (24.07%) LOSS: 0.4299\n",
            "ITER 30000 (24.90%) LOSS: 0.4397\n",
            "ITER 31000 (25.73%) LOSS: 0.4540\n",
            "ITER 32000 (26.56%) LOSS: 0.4478\n",
            "ITER 33000 (27.39%) LOSS: 0.4380\n",
            "ITER 34000 (28.22%) LOSS: 0.4416\n",
            "ITER 35000 (29.05%) LOSS: 0.4454\n",
            "ITER 36000 (29.88%) LOSS: 0.4647\n",
            "ITER 37000 (30.71%) LOSS: 0.4635\n",
            "ITER 38000 (31.54%) LOSS: 0.4351\n",
            "ITER 39000 (32.37%) LOSS: 0.4557\n",
            "ITER 40000 (33.20%) LOSS: 0.4369\n",
            "ITER 41000 (34.03%) LOSS: 0.4639\n",
            "ITER 42000 (34.86%) LOSS: 0.4638\n",
            "ITER 43000 (35.69%) LOSS: 0.4434\n",
            "ITER 44000 (36.52%) LOSS: 0.4719\n",
            "ITER 45000 (37.35%) LOSS: 0.4947\n",
            "ITER 46000 (38.18%) LOSS: 0.4553\n",
            "ITER 47000 (39.01%) LOSS: 0.4862\n",
            "ITER 48000 (39.84%) LOSS: 0.4893\n",
            "ITER 49000 (40.67%) LOSS: 0.5022\n",
            "ITER 50000 (41.50%) LOSS: 0.4636\n",
            "ITER 51000 (42.33%) LOSS: 0.4458\n",
            "ITER 52000 (43.16%) LOSS: 0.4442\n",
            "ITER 53000 (44.00%) LOSS: 0.4614\n",
            "ITER 54000 (44.83%) LOSS: 0.4852\n",
            "ITER 55000 (45.66%) LOSS: 0.4320\n",
            "ITER 56000 (46.49%) LOSS: 0.4451\n",
            "ITER 57000 (47.32%) LOSS: 0.4239\n",
            "ITER 58000 (48.15%) LOSS: 0.4565\n",
            "ITER 59000 (48.98%) LOSS: 0.4197\n",
            "ITER 60000 (49.81%) LOSS: 0.4668\n",
            "ITER 61000 (50.64%) LOSS: 0.4632\n",
            "ITER 62000 (51.47%) LOSS: 0.4448\n",
            "ITER 63000 (52.30%) LOSS: 0.4553\n",
            "ITER 64000 (53.13%) LOSS: 0.4474\n",
            "ITER 65000 (53.96%) LOSS: 0.4194\n",
            "ITER 66000 (54.79%) LOSS: 0.4762\n",
            "ITER 67000 (55.62%) LOSS: 0.4658\n",
            "ITER 68000 (56.45%) LOSS: 0.4408\n",
            "ITER 69000 (57.28%) LOSS: 0.4191\n",
            "ITER 70000 (58.11%) LOSS: 0.4516\n",
            "ITER 71000 (58.94%) LOSS: 0.4663\n",
            "ITER 72000 (59.77%) LOSS: 0.4543\n",
            "ITER 73000 (60.60%) LOSS: 0.4472\n",
            "ITER 74000 (61.43%) LOSS: 0.4527\n",
            "ITER 75000 (62.26%) LOSS: 0.4181\n",
            "ITER 76000 (63.09%) LOSS: 0.4680\n",
            "ITER 77000 (63.92%) LOSS: 0.4697\n",
            "ITER 78000 (64.75%) LOSS: 0.4604\n",
            "ITER 79000 (65.58%) LOSS: 0.4423\n",
            "ITER 80000 (66.41%) LOSS: 0.4335\n",
            "ITER 81000 (67.24%) LOSS: 0.4803\n",
            "ITER 82000 (68.07%) LOSS: 0.4483\n",
            "ITER 83000 (68.90%) LOSS: 0.4604\n",
            "ITER 84000 (69.73%) LOSS: 0.4435\n",
            "ITER 85000 (70.56%) LOSS: 0.4152\n",
            "ITER 86000 (71.39%) LOSS: 0.4354\n",
            "ITER 87000 (72.22%) LOSS: 0.4647\n",
            "ITER 88000 (73.05%) LOSS: 0.4063\n",
            "ITER 89000 (73.88%) LOSS: 0.4333\n",
            "ITER 90000 (74.71%) LOSS: 0.4391\n",
            "ITER 91000 (75.54%) LOSS: 0.4418\n",
            "ITER 92000 (76.37%) LOSS: 0.4611\n",
            "ITER 93000 (77.20%) LOSS: 0.4452\n",
            "ITER 94000 (78.03%) LOSS: 0.4115\n",
            "ITER 95000 (78.86%) LOSS: 0.4221\n",
            "ITER 96000 (79.69%) LOSS: 0.4316\n",
            "ITER 97000 (80.52%) LOSS: 0.4437\n",
            "ITER 98000 (81.35%) LOSS: 0.4283\n",
            "ITER 99000 (82.18%) LOSS: 0.4483\n",
            "ITER 100000 (83.01%) LOSS: 0.4365\n",
            "ITER 101000 (83.84%) LOSS: 0.4559\n",
            "ITER 102000 (84.67%) LOSS: 0.4448\n",
            "ITER 103000 (85.50%) LOSS: 0.4240\n",
            "ITER 104000 (86.33%) LOSS: 0.4587\n",
            "ITER 105000 (87.16%) LOSS: 0.4607\n",
            "ITER 106000 (87.99%) LOSS: 0.4576\n",
            "ITER 107000 (88.82%) LOSS: 0.4604\n",
            "ITER 108000 (89.65%) LOSS: 0.4558\n",
            "ITER 109000 (90.48%) LOSS: 0.4759\n",
            "ITER 110000 (91.31%) LOSS: 0.4516\n",
            "ITER 111000 (92.14%) LOSS: 0.4027\n",
            "ITER 112000 (92.97%) LOSS: 0.4412\n",
            "ITER 113000 (93.80%) LOSS: 0.4485\n",
            "ITER 114000 (94.63%) LOSS: 0.4676\n",
            "ITER 115000 (95.46%) LOSS: 0.4472\n",
            "ITER 116000 (96.29%) LOSS: 0.4445\n",
            "ITER 117000 (97.12%) LOSS: 0.4267\n",
            "ITER 118000 (97.95%) LOSS: 0.4531\n",
            "ITER 119000 (98.78%) LOSS: 0.4394\n",
            "ITER 120000 (99.61%) LOSS: 0.4449\n",
            "ACCURACY: 0.5058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdiniSTWIKpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5a05d36-2f75-42d6-84a0-69575659b6f2"
      },
      "source": [
        "train_epoch(encoder, decoder, learning_rate=0.0005, teacher_forcing_ratio=0.25)\n",
        "evaluate(encoder, decoder)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITER 1000 (0.83%) LOSS: 0.4095\n",
            "ITER 2000 (1.66%) LOSS: 0.4500\n",
            "ITER 3000 (2.49%) LOSS: 0.4188\n",
            "ITER 4000 (3.32%) LOSS: 0.4160\n",
            "ITER 5000 (4.15%) LOSS: 0.4137\n",
            "ITER 6000 (4.98%) LOSS: 0.4270\n",
            "ITER 7000 (5.81%) LOSS: 0.3983\n",
            "ITER 8000 (6.64%) LOSS: 0.4346\n",
            "ITER 9000 (7.47%) LOSS: 0.3959\n",
            "ITER 10000 (8.30%) LOSS: 0.4379\n",
            "ITER 11000 (9.13%) LOSS: 0.4335\n",
            "ITER 12000 (9.96%) LOSS: 0.4010\n",
            "ITER 13000 (10.79%) LOSS: 0.4654\n",
            "ITER 14000 (11.62%) LOSS: 0.4140\n",
            "ITER 15000 (12.45%) LOSS: 0.3956\n",
            "ITER 16000 (13.28%) LOSS: 0.4030\n",
            "ITER 17000 (14.11%) LOSS: 0.4254\n",
            "ITER 18000 (14.94%) LOSS: 0.4376\n",
            "ITER 19000 (15.77%) LOSS: 0.4184\n",
            "ITER 20000 (16.60%) LOSS: 0.4320\n",
            "ITER 21000 (17.43%) LOSS: 0.4104\n",
            "ITER 22000 (18.26%) LOSS: 0.3757\n",
            "ITER 23000 (19.09%) LOSS: 0.4257\n",
            "ITER 24000 (19.92%) LOSS: 0.4335\n",
            "ITER 25000 (20.75%) LOSS: 0.4043\n",
            "ITER 26000 (21.58%) LOSS: 0.3882\n",
            "ITER 27000 (22.41%) LOSS: 0.4219\n",
            "ITER 28000 (23.24%) LOSS: 0.4179\n",
            "ITER 29000 (24.07%) LOSS: 0.4153\n",
            "ITER 30000 (24.90%) LOSS: 0.4001\n",
            "ITER 31000 (25.73%) LOSS: 0.4379\n",
            "ITER 32000 (26.56%) LOSS: 0.4655\n",
            "ITER 33000 (27.39%) LOSS: 0.3727\n",
            "ITER 34000 (28.22%) LOSS: 0.3956\n",
            "ITER 35000 (29.05%) LOSS: 0.4339\n",
            "ITER 36000 (29.88%) LOSS: 0.4187\n",
            "ITER 37000 (30.71%) LOSS: 0.4148\n",
            "ITER 38000 (31.54%) LOSS: 0.4475\n",
            "ITER 39000 (32.37%) LOSS: 0.4073\n",
            "ITER 40000 (33.20%) LOSS: 0.4115\n",
            "ITER 41000 (34.03%) LOSS: 0.4086\n",
            "ITER 42000 (34.86%) LOSS: 0.4265\n",
            "ITER 43000 (35.69%) LOSS: 0.4281\n",
            "ITER 44000 (36.52%) LOSS: 0.4248\n",
            "ITER 45000 (37.35%) LOSS: 0.4350\n",
            "ITER 46000 (38.18%) LOSS: 0.4311\n",
            "ITER 47000 (39.01%) LOSS: 0.3981\n",
            "ITER 48000 (39.84%) LOSS: 0.4394\n",
            "ITER 49000 (40.67%) LOSS: 0.4310\n",
            "ITER 50000 (41.50%) LOSS: 0.4166\n",
            "ITER 51000 (42.33%) LOSS: 0.4154\n",
            "ITER 52000 (43.16%) LOSS: 0.4056\n",
            "ITER 53000 (44.00%) LOSS: 0.3957\n",
            "ITER 54000 (44.83%) LOSS: 0.4106\n",
            "ITER 55000 (45.66%) LOSS: 0.4379\n",
            "ITER 56000 (46.49%) LOSS: 0.4009\n",
            "ITER 57000 (47.32%) LOSS: 0.4122\n",
            "ITER 58000 (48.15%) LOSS: 0.4001\n",
            "ITER 59000 (48.98%) LOSS: 0.4418\n",
            "ITER 60000 (49.81%) LOSS: 0.4021\n",
            "ITER 61000 (50.64%) LOSS: 0.4197\n",
            "ITER 62000 (51.47%) LOSS: 0.3682\n",
            "ITER 63000 (52.30%) LOSS: 0.4058\n",
            "ITER 64000 (53.13%) LOSS: 0.4251\n",
            "ITER 65000 (53.96%) LOSS: 0.4257\n",
            "ITER 66000 (54.79%) LOSS: 0.4547\n",
            "ITER 67000 (55.62%) LOSS: 0.3934\n",
            "ITER 68000 (56.45%) LOSS: 0.3938\n",
            "ITER 69000 (57.28%) LOSS: 0.3969\n",
            "ITER 70000 (58.11%) LOSS: 0.4088\n",
            "ITER 71000 (58.94%) LOSS: 0.4073\n",
            "ITER 72000 (59.77%) LOSS: 0.4283\n",
            "ITER 73000 (60.60%) LOSS: 0.4187\n",
            "ITER 74000 (61.43%) LOSS: 0.3891\n",
            "ITER 75000 (62.26%) LOSS: 0.4219\n",
            "ITER 76000 (63.09%) LOSS: 0.4328\n",
            "ITER 77000 (63.92%) LOSS: 0.3937\n",
            "ITER 78000 (64.75%) LOSS: 0.4277\n",
            "ITER 79000 (65.58%) LOSS: 0.4038\n",
            "ITER 80000 (66.41%) LOSS: 0.4280\n",
            "ITER 81000 (67.24%) LOSS: 0.4578\n",
            "ITER 82000 (68.07%) LOSS: 0.4479\n",
            "ITER 83000 (68.90%) LOSS: 0.4246\n",
            "ITER 84000 (69.73%) LOSS: 0.3972\n",
            "ITER 85000 (70.56%) LOSS: 0.4147\n",
            "ITER 86000 (71.39%) LOSS: 0.3868\n",
            "ITER 87000 (72.22%) LOSS: 0.4049\n",
            "ITER 88000 (73.05%) LOSS: 0.4188\n",
            "ITER 89000 (73.88%) LOSS: 0.3995\n",
            "ITER 90000 (74.71%) LOSS: 0.4079\n",
            "ITER 91000 (75.54%) LOSS: 0.4307\n",
            "ITER 92000 (76.37%) LOSS: 0.4063\n",
            "ITER 93000 (77.20%) LOSS: 0.4267\n",
            "ITER 94000 (78.03%) LOSS: 0.4266\n",
            "ITER 95000 (78.86%) LOSS: 0.4192\n",
            "ITER 96000 (79.69%) LOSS: 0.4245\n",
            "ITER 97000 (80.52%) LOSS: 0.4311\n",
            "ITER 98000 (81.35%) LOSS: 0.3955\n",
            "ITER 99000 (82.18%) LOSS: 0.4085\n",
            "ITER 100000 (83.01%) LOSS: 0.4039\n",
            "ITER 101000 (83.84%) LOSS: 0.4114\n",
            "ITER 102000 (84.67%) LOSS: 0.4312\n",
            "ITER 103000 (85.50%) LOSS: 0.4465\n",
            "ITER 104000 (86.33%) LOSS: 0.4011\n",
            "ITER 105000 (87.16%) LOSS: 0.4121\n",
            "ITER 106000 (87.99%) LOSS: 0.4194\n",
            "ITER 107000 (88.82%) LOSS: 0.4304\n",
            "ITER 108000 (89.65%) LOSS: 0.4196\n",
            "ITER 109000 (90.48%) LOSS: 0.4109\n",
            "ITER 110000 (91.31%) LOSS: 0.4042\n",
            "ITER 111000 (92.14%) LOSS: 0.4003\n",
            "ITER 112000 (92.97%) LOSS: 0.4224\n",
            "ITER 113000 (93.80%) LOSS: 0.4313\n",
            "ITER 114000 (94.63%) LOSS: 0.3967\n",
            "ITER 115000 (95.46%) LOSS: 0.4294\n",
            "ITER 116000 (96.29%) LOSS: 0.4126\n",
            "ITER 117000 (97.12%) LOSS: 0.4309\n",
            "ITER 118000 (97.95%) LOSS: 0.4253\n",
            "ITER 119000 (98.78%) LOSS: 0.4376\n",
            "ITER 120000 (99.61%) LOSS: 0.4292\n",
            "ACCURACY: 0.5242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvV4qIQvoukq",
        "colab_type": "text"
      },
      "source": [
        "We save our current model to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rns4DZxEez-t",
        "colab_type": "code",
        "outputId": "787e8dd7-e846-49e4-b816-338a1b1b4a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "torch.save(encoder, '{}/My Drive/Colab Notebooks/hola/data/encoder-{}.pt'.format(mounted_folder_fp, hidden_size))\n",
        "torch.save(decoder, '{}/My Drive/Colab Notebooks/hola/data/decoder-{}.pt'.format(mounted_folder_fp, hidden_size))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JuUDyXSoxFK",
        "colab_type": "text"
      },
      "source": [
        "We load a model from file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mglUl-oqFFdl",
        "colab_type": "code",
        "outputId": "cd8cbe88-ab79-48dd-9104-a685ee35bc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "encoder = torch.load('{}/My Drive/Colab Notebooks/hola/data/encoder-{}.pt'.format(mounted_folder_fp, hidden_size))\n",
        "decoder = torch.load('{}/My Drive/Colab Notebooks/hola/data/decoder-{}.pt'.format(mounted_folder_fp, hidden_size))\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(120468, 256)\n",
              "  (gru): GRU(256, 256)\n",
              "  (out): Linear(in_features=256, out_features=120468, bias=True)\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}